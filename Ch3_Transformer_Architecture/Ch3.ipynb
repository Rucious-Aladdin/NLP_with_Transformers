{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNT5KrnEIofxxACUxSWWmt2"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"af0f27c8addc4c4e880b952247013a00":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_a4f2a999cc3b48faaf3885473ba61b26","IPY_MODEL_ec564b46687d45b2a80c7e005e2f1798","IPY_MODEL_7bcf1ea0b94d43a49edf4499e0e23a63"],"layout":"IPY_MODEL_4ad712c0abba4b5fb3b856f4d97c7539"}},"a4f2a999cc3b48faaf3885473ba61b26":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_25e59d128bda49f7aa7f93ee6224eb54","placeholder":"​","style":"IPY_MODEL_012c30fe9bba4880af3ea66802480231","value":"model.safetensors: 100%"}},"ec564b46687d45b2a80c7e005e2f1798":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_9c0769718a9a482bae34d14fc585e815","max":440449768,"min":0,"orientation":"horizontal","style":"IPY_MODEL_501708fbe0094f8d9ca61b38477476ef","value":440449768}},"7bcf1ea0b94d43a49edf4499e0e23a63":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_dc059e8b8ea04e0f8245ba781c53d798","placeholder":"​","style":"IPY_MODEL_f04d2b315e4f495d9ceb0b4730780b51","value":" 440M/440M [00:04&lt;00:00, 93.4MB/s]"}},"4ad712c0abba4b5fb3b856f4d97c7539":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"25e59d128bda49f7aa7f93ee6224eb54":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"012c30fe9bba4880af3ea66802480231":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"9c0769718a9a482bae34d14fc585e815":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"501708fbe0094f8d9ca61b38477476ef":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"dc059e8b8ea04e0f8245ba781c53d798":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f04d2b315e4f495d9ceb0b4730780b51":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RShPWYo01Fmi","executionInfo":{"status":"ok","timestamp":1711956380195,"user_tz":-540,"elapsed":20329,"user":{"displayName":"김수성","userId":"09570314636293598107"}},"outputId":"be208687-9318-405b-90ff-e9d41dd6423a"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount(\"/content/drive\")"]},{"cell_type":"code","source":["# 3.1 Transformer Architecture"],"metadata":{"id":"ToHiOLpL1OHK","executionInfo":{"status":"ok","timestamp":1711956380196,"user_tz":-540,"elapsed":8,"user":{"displayName":"김수성","userId":"09570314636293598107"}}},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":["### 인코더 유형\n","- BERT, RoBerta, DistilBERT\n","- text sequence 입력을 수치표현으로 변환\n","\n","### 디코더 유형\n","- GPT 계열의 생성모델이 이에 해당\n","- 왼쪽 문맥에 따라 다음 토큰을 순차적으로 계산\n","\n","### 인코더-디코더 유형\n","- 한 텍스트 시퀀스를 다른 시퀀스로 매핑함.\n","- BART, T5 모델이 이에 해당."],"metadata":{"id":"7zI-9GCY15lG"}},{"cell_type":"code","source":["#!pip install bertviz\n","from transformers import AutoTokenizer\n","from bertviz.transformers_neuron_view import BertModel\n","from bertviz.neuron_view import show"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PuJT5vdy22eD","executionInfo":{"status":"ok","timestamp":1711956872795,"user_tz":-540,"elapsed":492605,"user":{"displayName":"김수성","userId":"09570314636293598107"}},"outputId":"344819ce-9219-4781-83cf-899b868d58d7"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting bertviz\n","  Downloading bertviz-1.4.0-py3-none-any.whl (157 kB)\n","\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/157.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m30.7/157.6 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.2/157.6 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m157.6/157.6 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: transformers>=2.0 in /usr/local/lib/python3.10/dist-packages (from bertviz) (4.38.2)\n","Requirement already satisfied: torch>=1.0 in /usr/local/lib/python3.10/dist-packages (from bertviz) (2.2.1+cu121)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from bertviz) (4.66.2)\n","Collecting boto3 (from bertviz)\n","  Downloading boto3-1.34.74-py3-none-any.whl (139 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.3/139.3 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from bertviz) (2.31.0)\n","Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from bertviz) (2023.12.25)\n","Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from bertviz) (0.1.99)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.0->bertviz) (3.13.3)\n","Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0->bertviz) (4.10.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.0->bertviz) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.0->bertviz) (3.2.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0->bertviz) (3.1.3)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.0->bertviz) (2023.6.0)\n","Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=1.0->bertviz)\n","  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m49.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=1.0->bertviz)\n","  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m69.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=1.0->bertviz)\n","  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m50.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=1.0->bertviz)\n","  Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-cublas-cu12==12.1.3.1 (from torch>=1.0->bertviz)\n","  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-cufft-cu12==11.0.2.54 (from torch>=1.0->bertviz)\n","  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-curand-cu12==10.3.2.106 (from torch>=1.0->bertviz)\n","  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=1.0->bertviz)\n","  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=1.0->bertviz)\n","  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-nccl-cu12==2.19.3 (from torch>=1.0->bertviz)\n","  Downloading nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.0/166.0 MB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-nvtx-cu12==12.1.105 (from torch>=1.0->bertviz)\n","  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0->bertviz) (2.2.0)\n","Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.0->bertviz)\n","  Downloading nvidia_nvjitlink_cu12-12.4.99-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers>=2.0->bertviz) (0.20.3)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=2.0->bertviz) (1.25.2)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers>=2.0->bertviz) (24.0)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=2.0->bertviz) (6.0.1)\n","Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers>=2.0->bertviz) (0.15.2)\n","Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=2.0->bertviz) (0.4.2)\n","Collecting botocore<1.35.0,>=1.34.74 (from boto3->bertviz)\n","  Downloading botocore-1.34.74-py3-none-any.whl (12.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.1/12.1 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting jmespath<2.0.0,>=0.7.1 (from boto3->bertviz)\n","  Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n","Collecting s3transfer<0.11.0,>=0.10.0 (from boto3->bertviz)\n","  Downloading s3transfer-0.10.1-py3-none-any.whl (82 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.2/82.2 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->bertviz) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->bertviz) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->bertviz) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->bertviz) (2024.2.2)\n","Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.10/dist-packages (from botocore<1.35.0,>=1.34.74->boto3->bertviz) (2.8.2)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.0->bertviz) (2.1.5)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.0->bertviz) (1.3.0)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.35.0,>=1.34.74->boto3->bertviz) (1.16.0)\n","Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, jmespath, nvidia-cusparse-cu12, nvidia-cudnn-cu12, botocore, s3transfer, nvidia-cusolver-cu12, boto3, bertviz\n","Successfully installed bertviz-1.4.0 boto3-1.34.74 botocore-1.34.74 jmespath-1.0.1 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.4.99 nvidia-nvtx-cu12-12.1.105 s3transfer-0.10.1\n"]}]},{"cell_type":"code","source":["model_ckpt = \"bert-base-uncased\"\n","tokenizer = AutoTokenizer.from_pretrained(model_ckpt)\n","model = BertModel.from_pretrained(model_ckpt)\n","text = \"time flies like an arrow\"\n","show(model, \"bert\", tokenizer, text, display_mode=\"light\", layer=0, head=8)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":336,"output_embedded_package_id":"1vXxBwirKeZW2qAl5ouA2-pTru0tSNz4a"},"id":"d5ssLYdE36Bh","executionInfo":{"status":"ok","timestamp":1711956988682,"user_tz":-540,"elapsed":4337,"user":{"displayName":"김수성","userId":"09570314636293598107"}},"outputId":"4bae5518-8a28-4de7-9c57-e36cae859f40"},"execution_count":15,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]},{"cell_type":"code","source":["# Query = (seq_len, embedding_dim)\n","# Key = (embedding_dim, seq_len)\n","# qk = matmul(Query, Key) (seq_len, seq_len)\n","# softmax(qk / scaling_factor, axis=1), scaling_factor is broadcasted. (seq_len, seq_len)"],"metadata":{"id":"gYOTsd9b4uI_","executionInfo":{"status":"ok","timestamp":1711956988683,"user_tz":-540,"elapsed":14,"user":{"displayName":"김수성","userId":"09570314636293598107"}}},"execution_count":16,"outputs":[]},{"cell_type":"code","source":["\"\"\"\n","pytorch nn function\n","\n","nn.Linear,\n","nn.Module,\n","nn.Dropout,\n","nn.LayerNorm,\n","nn.Embedding\n","nn.GELU (Gaussian Error Linear Unit)\n","nn.bmm (batch matrix multiplication)\n","model.forward() -> pytorch forward method\n","\"\"\""],"metadata":{"id":"3S6w91dl6Ym0","colab":{"base_uri":"https://localhost:8080/","height":54},"executionInfo":{"status":"ok","timestamp":1711956988683,"user_tz":-540,"elapsed":13,"user":{"displayName":"김수성","userId":"09570314636293598107"}},"outputId":"37666585-5349-4689-feb6-49c1024dd62b"},"execution_count":17,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'\\npytorch nn function\\n\\nnn.Linear,\\nnn.Module,\\nnn.Dropout,\\nnn.LayerNorm,\\nnn.Embedding\\nnn.GELU (Gaussian Error Linear Unit)\\nnn.bmm (batch matrix multiplication)\\nmodel.forward() -> pytorch forward method\\n'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":17}]},{"cell_type":"markdown","source":["# 3.2 Encoder"],"metadata":{"id":"Waus3AKnvPUB"}},{"cell_type":"code","source":["inputs = tokenizer(text, padding=True, max_length=10, return_tensors=\"pt\", truncation=True)\n","config.max_length = 10"],"metadata":{"id":"qcZ2bX1-6zXf","executionInfo":{"status":"ok","timestamp":1711962412966,"user_tz":-540,"elapsed":306,"user":{"displayName":"김수성","userId":"09570314636293598107"}}},"execution_count":122,"outputs":[]},{"cell_type":"code","source":["def get_padded_sequence(ids, max_length):\n","    batch_size, seq_len = ids.shape\n","    padded_seq_ids = torch.zeros(size=(1, max_length), dtype=torch.long)\n","    padded_seq_ids[0, :seq_len] = ids\n","    return padded_seq_ids"],"metadata":{"id":"KD_0SmOM6622","executionInfo":{"status":"ok","timestamp":1711962413289,"user_tz":-540,"elapsed":8,"user":{"displayName":"김수성","userId":"09570314636293598107"}}},"execution_count":123,"outputs":[]},{"cell_type":"code","source":["import torch.nn as nn\n","from transformers import AutoConfig\n","\n","config = AutoConfig.from_pretrained(model_ckpt) # pretrained 임베딩 로드\n","token_emb = nn.Embedding(config.vocab_size, config.hidden_size) # embedding layer를 거쳐서 embedding으로 변환\n","print(token_emb) # (vocab_size, embedding_dim)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cGUEsgHB69Mc","executionInfo":{"status":"ok","timestamp":1711957753486,"user_tz":-540,"elapsed":715,"user":{"displayName":"김수성","userId":"09570314636293598107"}},"outputId":"a617264f-f438-425b-9010-3657d0683216"},"execution_count":50,"outputs":[{"output_type":"stream","name":"stdout","text":["Embedding(30522, 768)\n"]}]},{"cell_type":"code","source":["input_embeds = token_emb(inputs.input_ids)\n","input_embeds.size() # (batch_size, seq_size, embedding_dim)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gL8Melf77SjT","executionInfo":{"status":"ok","timestamp":1711962493716,"user_tz":-540,"elapsed":340,"user":{"displayName":"김수성","userId":"09570314636293598107"}},"outputId":"61664e61-2ba5-4a6f-9fbf-9ca6e4a5e299"},"execution_count":126,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([1, 10, 768])"]},"metadata":{},"execution_count":126}]},{"cell_type":"code","source":["# calculating normalizaing factor\n","import torch\n","from math import sqrt\n","\n","query = key = value = input_embeds # query == key == value in encoder-self-attnetion case\n","dim_k = key.size(-1) # == embedding_dim\n","scores = torch.bmm(query, key.transpose(1, 2)) / sqrt(dim_k)\n","# (batch_size, seq_size, embedding_dim) * (batch_size, embedding_dim, seq_size) -> (batch_size, seq_size, seq_size)\n","\n","print(scores.size())\n","print(scores)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"u1hwq4OY7v7r","executionInfo":{"status":"ok","timestamp":1711957753890,"user_tz":-540,"elapsed":9,"user":{"displayName":"김수성","userId":"09570314636293598107"}},"outputId":"88f8dfab-c9d5-45f2-c64e-2b89fa7979cd"},"execution_count":52,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([1, 7, 7])\n","tensor([[[ 2.5603e+01,  6.4121e-01,  3.3966e-01, -1.5425e+00, -1.5452e+00,\n","           5.8863e-02,  2.9096e-01],\n","         [ 6.4121e-01,  2.7255e+01, -2.0742e-01,  1.2523e+00, -1.4954e+00,\n","          -3.6402e-01, -1.3188e-02],\n","         [ 3.3966e-01, -2.0742e-01,  2.5836e+01,  3.9427e-02,  4.5119e-01,\n","           4.7003e-01,  1.4218e+00],\n","         [-1.5425e+00,  1.2523e+00,  3.9427e-02,  2.6389e+01, -3.1081e-02,\n","          -5.1921e-01,  8.0605e-01],\n","         [-1.5452e+00, -1.4954e+00,  4.5119e-01, -3.1081e-02,  2.9287e+01,\n","          -5.8324e-01,  5.8137e-01],\n","         [ 5.8863e-02, -3.6402e-01,  4.7003e-01, -5.1921e-01, -5.8324e-01,\n","           2.6304e+01,  1.1723e+00],\n","         [ 2.9096e-01, -1.3187e-02,  1.4218e+00,  8.0605e-01,  5.8137e-01,\n","           1.1723e+00,  2.6959e+01]]], grad_fn=<DivBackward0>)\n"]}]},{"cell_type":"code","source":["import torch.nn.functional as F\n","\n","weights = F.softmax(scores, dim=-1)\n","print(weights.sum(dim=-1))\n","print(weights)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nz3ck2cY8Taa","executionInfo":{"status":"ok","timestamp":1711957753890,"user_tz":-540,"elapsed":5,"user":{"displayName":"김수성","userId":"09570314636293598107"}},"outputId":"0e7f2759-5dd9-44e3-cd85-6d7cdcaeac08"},"execution_count":53,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[1., 1., 1., 1., 1., 1., 1.]], grad_fn=<SumBackward1>)\n","tensor([[[1.0000e+00, 1.4431e-11, 1.0674e-11, 1.6252e-12, 1.6209e-12,\n","          8.0608e-12, 1.0167e-11],\n","         [2.7653e-12, 1.0000e+00, 1.1835e-12, 5.0947e-12, 3.2645e-13,\n","          1.0120e-12, 1.4372e-12],\n","         [8.4566e-12, 4.8933e-12, 1.0000e+00, 6.2634e-12, 9.4545e-12,\n","          9.6342e-12, 2.4954e-11],\n","         [7.4076e-13, 1.2119e-11, 3.6034e-12, 1.0000e+00, 3.3581e-12,\n","          2.0611e-12, 7.7563e-12],\n","         [4.0713e-14, 4.2792e-14, 2.9974e-13, 1.8505e-13, 1.0000e+00,\n","          1.0654e-13, 3.4142e-13],\n","         [3.9973e-12, 2.6189e-12, 6.0303e-12, 2.2424e-12, 2.1033e-12,\n","          1.0000e+00, 1.2171e-11],\n","         [2.6199e-12, 1.9328e-12, 8.1167e-12, 4.3852e-12, 3.5027e-12,\n","          6.3248e-12, 1.0000e+00]]], grad_fn=<SoftmaxBackward0>)\n"]}]},{"cell_type":"code","source":["# 이어텐션의 문제점. -> 같은 embedding vector의 부호가 같기 때문에 softmax 값이 1이 나오게 되고 attention을 진행하는 의미가 없음."],"metadata":{"id":"1SHn2w2y_8fq","executionInfo":{"status":"ok","timestamp":1711957754528,"user_tz":-540,"elapsed":2,"user":{"displayName":"김수성","userId":"09570314636293598107"}}},"execution_count":54,"outputs":[]},{"cell_type":"code","source":["def scaled_dot_product_attention(query, key, value, mask=None, pad_mask=None):\n","  dim_k = query.size(-1)\n","  scores = torch.bmm(query, key.transpose(1, 2)) / sqrt(dim_k)\n","  if mask is not None:\n","    scores = scores.masked_fill(mask==0, float(-1e10))\n","  if pad_mask is not None:\n","    scores = scores.masked_fill(pad_mask==0, float(-1e10))\n","  weights = torch.exp(F.log_softmax(scores, dim=-1))\n","  return weights.bmm(value) # (batch_size, seq_size, seq_size), (batch_size, seq_size, embedding_dim) -> weighted sum with respect to attention vector"],"metadata":{"id":"X8O2b8LJ80UU","executionInfo":{"status":"ok","timestamp":1711966622876,"user_tz":-540,"elapsed":433,"user":{"displayName":"김수성","userId":"09570314636293598107"}}},"execution_count":192,"outputs":[]},{"cell_type":"code","source":["def make_padding_mask(q, k, padding_idx):\n","    # q,k의 size = (batch_size, seq_len)\n","    _, q_seq_len = q.size()\n","    _, k_seq_len = k.size()\n","\n","    q = q.ne(padding_idx)  # padding token을 0, 나머지를 1로 만들어줌\n","    q = q.unsqueeze(1).unsqueeze(3) # (batch_size, 1, q_seq_len, 1)\n","    q = q.repeat(1,1,1,k_seq_len)   # (batch_size, 1, q_seq_len, k_seq_len)\n","\n","    k = k.ne(padding_idx)\n","    k = k.unsqueeze(1).unsqueeze(2) # (batch_size, 1, 1, k_seq_len)\n","    k = k.repeat(1,1,q_seq_len,1)   # (batch_size, 1, q_seq_len, k_seq_len)\n","\n","    # and 연산\n","    # (batch_size, 1, q_seq_len, k_seq_len)\n","    mask = q & k\n","\n","    return mask.int().squeeze(dim=1)"],"metadata":{"id":"3ieD_JqscnRo","executionInfo":{"status":"ok","timestamp":1711963688296,"user_tz":-540,"elapsed":383,"user":{"displayName":"김수성","userId":"09570314636293598107"}}},"execution_count":149,"outputs":[]},{"cell_type":"code","source":["# multi-head-attention\n","class AttentionHead(nn.Module):\n","  def __init__(self, embed_dim, head_dim, mask=False, padding_idx=None):\n","    super(AttentionHead, self).__init__()\n","    self.q = nn.Linear(embed_dim, head_dim)\n","    self.k = nn.Linear(embed_dim, head_dim)\n","    self.v = nn.Linear(embed_dim, head_dim)\n","    self.mask = mask\n","    self.padding_idx= padding_idx\n","\n","  def forward(self, hidden_state, padding_mask=None):\n","\n","    if self.mask:\n","      mask_matrix = torch.tril(torch.ones(hidden_state.size(1), hidden_state.size(1))).unsqueeze(0)\n","      mask_tensor = mask_matrix.repeat(hidden_state.size(0), 1, 1)\n","      mask_tensor.require_grads=False\n","    else:\n","      mask_tensor = None\n","    attn_outputs = scaled_dot_product_attention(\n","        self.q(hidden_state), self.k(hidden_state), self.v(hidden_state), mask_tensor, padding_mask\n","    )\n","\n","    return attn_outputs\n"],"metadata":{"id":"qvWpiuafAegy","executionInfo":{"status":"ok","timestamp":1711957756157,"user_tz":-540,"elapsed":2,"user":{"displayName":"김수성","userId":"09570314636293598107"}}},"execution_count":57,"outputs":[]},{"cell_type":"code","source":["class MultiHeadAttention(nn.Module):\n","  def __init__(self, config, mask=False):\n","    super(MultiHeadAttention, self).__init__()\n","    embed_dim = config.hidden_size\n","    num_heads = config.num_attention_heads\n","    head_dim = embed_dim // num_heads\n","    padding_idx = config.pad_token_id\n","    self.mask = mask\n","    self.heads = nn.ModuleList(\n","        [AttentionHead(embed_dim, head_dim, mask, padding_idx) for _ in range(num_heads)]\n","    )\n","    self.output_linear= nn.Linear(embed_dim, embed_dim)\n","\n","  def forward(self, hidden_state, padding_mask=None):\n","    x = torch.cat([h(hidden_state, padding_mask) for h in self.heads], dim=-1)\n","    x = self.output_linear(x)\n","    return x"],"metadata":{"id":"RnxS5m7CBfwn","executionInfo":{"status":"ok","timestamp":1711957756453,"user_tz":-540,"elapsed":4,"user":{"displayName":"김수성","userId":"09570314636293598107"}}},"execution_count":58,"outputs":[]},{"cell_type":"code","source":["multihead_attn = MultiHeadAttention(config)\n","attn_output = multihead_attn(input_embeds)\n","attn_output.size()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IQIdq_H_CQcZ","executionInfo":{"status":"ok","timestamp":1711957757574,"user_tz":-540,"elapsed":3,"user":{"displayName":"김수성","userId":"09570314636293598107"}},"outputId":"7598ab5d-475d-4211-953f-99745d4bbc6d"},"execution_count":59,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([1, 7, 768])"]},"metadata":{},"execution_count":59}]},{"cell_type":"code","source":["# Self-Attention Visualization\n","\n","from bertviz import head_view\n","from transformers import AutoModel\n","\n","model = AutoModel.from_pretrained(model_ckpt, output_attentions=True)\n","\n","sentence_a = \"time flies like an arrow\"\n","sentence_b = \"fruit flies like a banana\"\n","\n","viz_inputs = tokenizer(sentence_a, sentence_b, return_tensors=\"pt\")\n","attention = model(**viz_inputs).attentions\n","sentence_b_start = (viz_inputs.token_type_ids == 0).sum(dim=1)\n","tokens = tokenizer.convert_ids_to_tokens(viz_inputs.input_ids[0])\n","\n","head_view(attention, tokens, sentence_b_start, heads=[1])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":393,"output_embedded_package_id":"1JEid9pSlGggkdAkhBy_PMQ2Io_ypN9cR","referenced_widgets":["af0f27c8addc4c4e880b952247013a00","a4f2a999cc3b48faaf3885473ba61b26","ec564b46687d45b2a80c7e005e2f1798","7bcf1ea0b94d43a49edf4499e0e23a63","4ad712c0abba4b5fb3b856f4d97c7539","25e59d128bda49f7aa7f93ee6224eb54","012c30fe9bba4880af3ea66802480231","9c0769718a9a482bae34d14fc585e815","501708fbe0094f8d9ca61b38477476ef","dc059e8b8ea04e0f8245ba781c53d798","f04d2b315e4f495d9ceb0b4730780b51"]},"id":"uDZC-tQzCYf0","executionInfo":{"status":"ok","timestamp":1711957767525,"user_tz":-540,"elapsed":8855,"user":{"displayName":"김수성","userId":"09570314636293598107"}},"outputId":"093e5c4b-a0be-4243-8c04-334481300af5"},"execution_count":60,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]},{"cell_type":"code","source":["class FeedForward(nn.Module):\n","  def __init__(self, config):\n","    super().__init__()\n","    self.linear_1 = nn.Linear(config.hidden_size, config.intermediate_size)\n","    self.linear_2 = nn.Linear(config.intermediate_size, config.hidden_size)\n","    self.gelu = nn.GELU()\n","    self.dropout = nn.Dropout(config.hidden_dropout_prob)\n","\n","  def forward(self, x):\n","    x = self.linear_1(x)\n","    x = self.gelu(x)\n","    x = self.linear_2(x)\n","    x = self.dropout(x)\n","    return x"],"metadata":{"id":"FwWzXCOpDlGJ","executionInfo":{"status":"ok","timestamp":1711957767525,"user_tz":-540,"elapsed":6,"user":{"displayName":"김수성","userId":"09570314636293598107"}}},"execution_count":61,"outputs":[]},{"cell_type":"code","source":["feed_forward = FeedForward(config)\n","ff_outputs = feed_forward(attn_output)\n","ff_outputs.size()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zVYEKSS2G_s9","executionInfo":{"status":"ok","timestamp":1711957767525,"user_tz":-540,"elapsed":6,"user":{"displayName":"김수성","userId":"09570314636293598107"}},"outputId":"d7a7d537-c1de-4f9c-8303-442631f85c66"},"execution_count":62,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([1, 7, 768])"]},"metadata":{},"execution_count":62}]},{"cell_type":"code","source":["class TransformerEncoderLayer(nn.Module):\n","  def __init__(self, config):\n","    super().__init__()\n","    self.layer_norm_1 = nn.LayerNorm(config.hidden_size)\n","    self.layer_norm_2 = nn.LayerNorm(config.hidden_size)\n","    self.attention = MultiHeadAttention(config)\n","    self.feed_forward = FeedForward(config)\n","    self.padding_idx = config.pad_token_id\n","\n","\n","  def forward(self, x, padding_mask=None):\n","    hidden_state = self.layer_norm_1(x)\n","    x = x + self.attention(hidden_state, padding_mask)\n","    x = x + self.feed_forward(self.layer_norm_2(x))\n","    return x"],"metadata":{"id":"L7AeL4HrHK0g","executionInfo":{"status":"ok","timestamp":1711957767525,"user_tz":-540,"elapsed":4,"user":{"displayName":"김수성","userId":"09570314636293598107"}}},"execution_count":63,"outputs":[]},{"cell_type":"code","source":["encoder_layer = TransformerEncoderLayer(config)\n","input_embeds.shape, encoder_layer(input_embeds).size()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XMSbKQ0JljRP","executionInfo":{"status":"ok","timestamp":1711957767849,"user_tz":-540,"elapsed":6,"user":{"displayName":"김수성","userId":"09570314636293598107"}},"outputId":"62dad8bb-0279-4fe4-b029-1363cd346a29"},"execution_count":64,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(torch.Size([1, 7, 768]), torch.Size([1, 7, 768]))"]},"metadata":{},"execution_count":64}]},{"cell_type":"code","source":["class Embeddings(nn.Module):\n","  def __init__(self, config):\n","    super().__init__()\n","    self.token_embeddings = nn.Embedding(config.vocab_size,\n","                                         config.hidden_size,\n","                                         padding_idx=0)\n","    #위치도 니가 알아서 찾아봐라 식.\n","    self.position_embeddings = nn.Embedding(config.max_position_embeddings,\n","                                            config.hidden_size)\n","    self.layer_norm = nn.LayerNorm(config.hidden_size, eps=1e-12)\n","    self.dropout = nn.Dropout()\n","\n","  def forward(self, input_ids):\n","    seq_length = input_ids.size(1)\n","    position_ids = torch.arange(seq_length, dtype=torch.long).unsqueeze(0)\n","\n","    token_embeddings = self.token_embeddings(input_ids)\n","    position_embeddings = self.position_embeddings(position_ids)\n","\n","    # token embedding과 position embedding을 단순히 더하는 연산인 것임.\n","    embeddings = token_embeddings + position_embeddings\n","    embeddings = self.layer_norm(embeddings)\n","    embeddings = self.dropout(embeddings)\n","    return embeddings"],"metadata":{"id":"67kzjnZCl0X8","executionInfo":{"status":"ok","timestamp":1711957767849,"user_tz":-540,"elapsed":5,"user":{"displayName":"김수성","userId":"09570314636293598107"}}},"execution_count":65,"outputs":[]},{"cell_type":"code","source":["embedding_layer = Embeddings(config)\n","embedding_layer(inputs.input_ids).size()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"g3mTuYmYpTYH","executionInfo":{"status":"ok","timestamp":1711961549180,"user_tz":-540,"elapsed":636,"user":{"displayName":"김수성","userId":"09570314636293598107"}},"outputId":"68da63e9-8474-4071-c17e-7162d982fce2"},"execution_count":108,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([1, 30, 768])"]},"metadata":{},"execution_count":108}]},{"cell_type":"markdown","source":["# Encoder의 구현\n","\n","- Embeddings Layer(token_embedding + position embedding +layer Normalization + Dropout)\n","- Transformer Encoder Layer(layerNorm + MultiHeadAttention + LayerNorm + Feed_ForwardNet)\n","- Transformer Encoder Layer를 겹으로 쌓아서 구성"],"metadata":{"id":"5cpLJfS4tN8i"}},{"cell_type":"code","source":["class TransformerEncoder(nn.Module):\n","  def __init__(self, config):\n","    super().__init__()\n","    self.embeddings = Embeddings(config)\n","    self.layers = nn.ModuleList(\n","        [TransformerEncoderLayer(config)\n","        for _ in range(config.num_hidden_layers)]\n","    )\n","    self.padding_idx = config.pad_token_id\n","    self.padding_mask = None\n","\n","  def forward(self, x):\n","    self.padding_mask = make_padding_mask(x, x, self.padding_idx)\n","    x = self.embeddings(x)\n","    for i, layer in enumerate(self.layers):\n","      #print(f\"{i + 1}번째 Layer\")\n","      x = layer(x, self.padding_mask)\n","    return x"],"metadata":{"id":"Q3WYH0g1pc6w","executionInfo":{"status":"ok","timestamp":1711965277919,"user_tz":-540,"elapsed":332,"user":{"displayName":"김수성","userId":"09570314636293598107"}}},"execution_count":171,"outputs":[]},{"cell_type":"code","source":["encoder = TransformerEncoder(config)\n","output = encoder(inputs.input_ids)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nRSnO0Wuq7xO","executionInfo":{"status":"ok","timestamp":1711964489256,"user_tz":-540,"elapsed":605,"user":{"displayName":"김수성","userId":"09570314636293598107"}},"outputId":"4d956dc6-59b5-4f97-9d2f-9c502feb318c"},"execution_count":161,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[[1, 1, 1, 1, 1, 1, 1, 0, 0, 0],\n","         [1, 1, 1, 1, 1, 1, 1, 0, 0, 0],\n","         [1, 1, 1, 1, 1, 1, 1, 0, 0, 0],\n","         [1, 1, 1, 1, 1, 1, 1, 0, 0, 0],\n","         [1, 1, 1, 1, 1, 1, 1, 0, 0, 0],\n","         [1, 1, 1, 1, 1, 1, 1, 0, 0, 0],\n","         [1, 1, 1, 1, 1, 1, 1, 0, 0, 0],\n","         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n","         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n","         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]], dtype=torch.int32)\n"]}]},{"cell_type":"code","source":["output.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7XNJRvQYn4PQ","executionInfo":{"status":"ok","timestamp":1711964491289,"user_tz":-540,"elapsed":3,"user":{"displayName":"김수성","userId":"09570314636293598107"}},"outputId":"a4001534-69be-4d3e-f043-83fa72e3a973"},"execution_count":162,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([1, 10, 768])"]},"metadata":{},"execution_count":162}]},{"cell_type":"code","source":["print(inputs.input_ids)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QIxxvqvB3VF8","executionInfo":{"status":"ok","timestamp":1711964491592,"user_tz":-540,"elapsed":3,"user":{"displayName":"김수성","userId":"09570314636293598107"}},"outputId":"25141d82-ac09-49da-9217-516f609b01b6"},"execution_count":163,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[  101,  2051, 10029,  2066,  2019,  8612,   102,     0,     0,     0]])\n"]}]},{"cell_type":"code","source":["class TransformerForSequenceClassification(nn.Module):\n","  def __init__(self, config):\n","    super().__init__()\n","    self.encoder = TransformerEncoder(config)\n","    self.dropout = nn.Dropout(config.hidden_dropout_prob)\n","    self.classifier = nn.Linear(config.hidden_size, config.num_labels)\n","\n","  def forward(self, x):\n","    x = self.encoder(x)[:, 0, :]\n","    x = self.dropout(x)\n","    x = self.classifier(x)\n","    return x"],"metadata":{"id":"zsf7zJV_rE5j","executionInfo":{"status":"ok","timestamp":1711807663161,"user_tz":-540,"elapsed":278,"user":{"displayName":"김수성","userId":"09570314636293598107"}}},"execution_count":29,"outputs":[]},{"cell_type":"code","source":["config.num_labels = 3\n","encoder_classifier = TransformerForSequenceClassification(config)\n","output = encoder_classifier(inputs.input_ids)"],"metadata":{"id":"ezIydzIIsAgt","executionInfo":{"status":"ok","timestamp":1711807778416,"user_tz":-540,"elapsed":1922,"user":{"displayName":"김수성","userId":"09570314636293598107"}}},"execution_count":34,"outputs":[]},{"cell_type":"code","source":["print(output)\n","output = F.softmax(output, dim =-1) # 정규화 되지 않은 logit을 반환"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DLmPmfb6sPt0","executionInfo":{"status":"ok","timestamp":1711807836048,"user_tz":-540,"elapsed":3,"user":{"displayName":"김수성","userId":"09570314636293598107"}},"outputId":"836c9f15-855b-46bb-f2b9-2fd41ce7bdff"},"execution_count":38,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[ 3.1588,  0.7398, -1.3600]], grad_fn=<AddmmBackward0>)\n"]}]},{"cell_type":"code","source":["output"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-8bBl__nsaFW","executionInfo":{"status":"ok","timestamp":1711807844038,"user_tz":-540,"elapsed":3,"user":{"displayName":"김수성","userId":"09570314636293598107"}},"outputId":"ccc92276-d0bf-4a1b-cd35-30f24205857c"},"execution_count":39,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[0.9092, 0.0809, 0.0099]], grad_fn=<SoftmaxBackward0>)"]},"metadata":{},"execution_count":39}]},{"cell_type":"markdown","source":["# 3.3 Decoder\n","- Masked Self-attention Layer\n","- Encoder-Decoder attention layer"],"metadata":{"id":"I3JQoP9au9W2"}},{"cell_type":"code","source":["seq_len = inputs.input_ids.size(-1)\n","mask = torch.tril(torch.ones(seq_len, seq_len).unsqueeze(0))\n","mask[0]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"01AIKcbgssx5","executionInfo":{"status":"ok","timestamp":1711964532887,"user_tz":-540,"elapsed":9,"user":{"displayName":"김수성","userId":"09570314636293598107"}},"outputId":"03cfbd5c-9655-459c-a86f-243fb41a0a0a"},"execution_count":167,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n","        [1., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n","        [1., 1., 1., 0., 0., 0., 0., 0., 0., 0.],\n","        [1., 1., 1., 1., 0., 0., 0., 0., 0., 0.],\n","        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n","        [1., 1., 1., 1., 1., 1., 0., 0., 0., 0.],\n","        [1., 1., 1., 1., 1., 1., 1., 0., 0., 0.],\n","        [1., 1., 1., 1., 1., 1., 1., 1., 0., 0.],\n","        [1., 1., 1., 1., 1., 1., 1., 1., 1., 0.],\n","        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]])"]},"metadata":{},"execution_count":167}]},{"cell_type":"code","source":["mask.masked_fill(mask==0, float(-1e10))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cGt_jYVM3v6D","executionInfo":{"status":"ok","timestamp":1711964567333,"user_tz":-540,"elapsed":5,"user":{"displayName":"김수성","userId":"09570314636293598107"}},"outputId":"9e3536ad-1495-4d70-d234-68f0264a13b4"},"execution_count":169,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[[ 1.0000e+00, -1.0000e+10, -1.0000e+10, -1.0000e+10, -1.0000e+10,\n","          -1.0000e+10, -1.0000e+10, -1.0000e+10, -1.0000e+10, -1.0000e+10],\n","         [ 1.0000e+00,  1.0000e+00, -1.0000e+10, -1.0000e+10, -1.0000e+10,\n","          -1.0000e+10, -1.0000e+10, -1.0000e+10, -1.0000e+10, -1.0000e+10],\n","         [ 1.0000e+00,  1.0000e+00,  1.0000e+00, -1.0000e+10, -1.0000e+10,\n","          -1.0000e+10, -1.0000e+10, -1.0000e+10, -1.0000e+10, -1.0000e+10],\n","         [ 1.0000e+00,  1.0000e+00,  1.0000e+00,  1.0000e+00, -1.0000e+10,\n","          -1.0000e+10, -1.0000e+10, -1.0000e+10, -1.0000e+10, -1.0000e+10],\n","         [ 1.0000e+00,  1.0000e+00,  1.0000e+00,  1.0000e+00,  1.0000e+00,\n","          -1.0000e+10, -1.0000e+10, -1.0000e+10, -1.0000e+10, -1.0000e+10],\n","         [ 1.0000e+00,  1.0000e+00,  1.0000e+00,  1.0000e+00,  1.0000e+00,\n","           1.0000e+00, -1.0000e+10, -1.0000e+10, -1.0000e+10, -1.0000e+10],\n","         [ 1.0000e+00,  1.0000e+00,  1.0000e+00,  1.0000e+00,  1.0000e+00,\n","           1.0000e+00,  1.0000e+00, -1.0000e+10, -1.0000e+10, -1.0000e+10],\n","         [ 1.0000e+00,  1.0000e+00,  1.0000e+00,  1.0000e+00,  1.0000e+00,\n","           1.0000e+00,  1.0000e+00,  1.0000e+00, -1.0000e+10, -1.0000e+10],\n","         [ 1.0000e+00,  1.0000e+00,  1.0000e+00,  1.0000e+00,  1.0000e+00,\n","           1.0000e+00,  1.0000e+00,  1.0000e+00,  1.0000e+00, -1.0000e+10],\n","         [ 1.0000e+00,  1.0000e+00,  1.0000e+00,  1.0000e+00,  1.0000e+00,\n","           1.0000e+00,  1.0000e+00,  1.0000e+00,  1.0000e+00,  1.0000e+00]]])"]},"metadata":{},"execution_count":169}]},{"cell_type":"markdown","source":["## Encoder-Decoder 전용 Attention"],"metadata":{"id":"57avxE4fL1J9"}},{"cell_type":"code","source":["# multi-head-attention\n","class AttentionEncDecHead(nn.Module):\n","  def __init__(self, embed_dim, head_dim, padding_idx=None, mask=False):\n","    super().__init__()\n","    self.q = nn.Linear(embed_dim, head_dim)\n","    self.k = nn.Linear(embed_dim, head_dim)\n","    self.v = nn.Linear(embed_dim, head_dim)\n","    self.mask = mask\n","    self.padding_idx=padding_idx\n","\n","  def forward(self, dec_hidden_state, enc_hidden_state, padding_mask=None):\n","    if self.mask:\n","      mask_matrix = torch.tril(torch.ones(enc_hidden_state.size(1), enc_hidden_state.size(1))).unsqueeze(0)\n","      mask_tensor = mask_matrix.repeat(enc_hidden_state.size(0), 1, 1)\n","      mask_tensor.require_grads=False\n","    else:\n","      mask_tensor = None\n","\n","    attn_outputs = scaled_dot_product_attention(\n","        self.q(dec_hidden_state), self.k(enc_hidden_state), self.v(enc_hidden_state), mask_tensor, padding_mask\n","    )\n","    return attn_outputs\n"],"metadata":{"id":"y-0bE9D9KA0u","executionInfo":{"status":"ok","timestamp":1711965755030,"user_tz":-540,"elapsed":374,"user":{"displayName":"김수성","userId":"09570314636293598107"}}},"execution_count":173,"outputs":[]},{"cell_type":"code","source":["class MultiHeadEncDecAttention(nn.Module):\n","  def __init__(self, config, mask=False):\n","    super().__init__()\n","    embed_dim = config.hidden_size\n","    num_heads = config.num_attention_heads\n","    head_dim = embed_dim // num_heads\n","    self.padding_idx = config.pad_token_id\n","    self.mask = mask\n","    self.heads = nn.ModuleList(\n","        [AttentionEncDecHead(embed_dim, head_dim, self.mask, self.padding_idx) for _ in range(num_heads)]\n","    )\n","    self.output_linear= nn.Linear(embed_dim, embed_dim)\n","\n","  def forward(self, dec_hidden_state, enc_hidden_state, padding_mask):\n","    x = torch.cat([h(dec_hidden_state, enc_hidden_state, padding_mask) for h in self.heads], dim=-1)\n","    x = self.output_linear(x)\n","    return x"],"metadata":{"id":"Z2N2RqI0J8aE","executionInfo":{"status":"ok","timestamp":1711965755347,"user_tz":-540,"elapsed":7,"user":{"displayName":"김수성","userId":"09570314636293598107"}}},"execution_count":174,"outputs":[]},{"cell_type":"markdown","source":["## Decoder Architecture\n","##### 여기서 부터는 직접하란다.. 이게 말이되냐.."],"metadata":{"id":"A21NfVx48MxB"}},{"cell_type":"code","source":["class TransformerDecoderLayer(nn.Module):\n","  def __init__(self, config):\n","    super().__init__()\n","    self.layer_norm_1 = nn.LayerNorm(config.hidden_size)\n","    self.layer_norm_2 = nn.LayerNorm(config.hidden_size)\n","    self.layer_norm_3 = nn.LayerNorm(config.hidden_size)\n","    self.layer_norm_4 = nn.LayerNorm(config.hidden_size)\n","    self.masked_attention = MultiHeadAttention(config, mask=True)\n","    self.enc_dec_attention = MultiHeadEncDecAttention(config, mask=True)\n","    self.feed_forward = FeedForward(config)\n","\n","  def forward(self, x, enc_outputs, attn_padding_mask=None, enc_dec_attn_padding_mask=None):\n","    hidden_state = self.layer_norm_1(x)\n","    x = x + self.masked_attention(hidden_state, attn_padding_mask)\n","    x = x + self.enc_dec_attention(self.layer_norm_2(x), self.layer_norm_3(enc_outputs), enc_dec_attn_padding_mask)\n","    x = x + self.feed_forward(self.layer_norm_4(x))\n","    return x"],"metadata":{"id":"UtIz5-9z4Wzp","executionInfo":{"status":"ok","timestamp":1711965763096,"user_tz":-540,"elapsed":334,"user":{"displayName":"김수성","userId":"09570314636293598107"}}},"execution_count":175,"outputs":[]},{"cell_type":"code","source":["class TransformerDecoder(nn.Module):\n","  def __init__(self, config):\n","    super().__init__()\n","    self.embeddings = Embeddings(config)\n","    self.layers = nn.ModuleList(\n","        [TransformerDecoderLayer(config)\n","        for _ in range(config.num_hidden_layers)]\n","    )\n","    self.padding_idx = config.pad_token_id\n","\n","  def forward(self, x_target, x_enc, enc_ids):\n","    #이 패딩마스크는 self-attention에만 적용됨.\n","    self.attn_padding_mask = make_padding_mask(x_target, x_target, self.padding_idx)\n","\n","    # enc-dec attention의 경우 padding_mask/look_ahead mask를 병합하는 추가연산을 필요로함.\n","    self.enc_dec_padding_mask = make_padding_mask(x_target, enc_ids, self.padding_idx)\n","\n","    #ids를 이용해 mask를 만든후에 embedding 진행\n","    x_target = self.embeddings(x_target)\n","    for layer in self.layers:\n","      x_target = layer(x_target, x_enc, self.attn_padding_mask, self.enc_dec_padding_mask)\n","    return x_target"],"metadata":{"id":"UtDYvRk95Gxg","executionInfo":{"status":"ok","timestamp":1711966410514,"user_tz":-540,"elapsed":4,"user":{"displayName":"김수성","userId":"09570314636293598107"}}},"execution_count":187,"outputs":[]},{"cell_type":"code","source":["# @title Encoder output\n","x_src_ids = inputs.input_ids\n","x_tgt_ids = inputs.input_ids\n","encoder_model = TransformerEncoder(config)\n","enc_output = encoder(inputs.input_ids)"],"metadata":{"id":"aw0J0yKQExvY","executionInfo":{"status":"ok","timestamp":1711966414030,"user_tz":-540,"elapsed":2071,"user":{"displayName":"김수성","userId":"09570314636293598107"}}},"execution_count":188,"outputs":[]},{"cell_type":"code","source":["# @title Decoder output\n","decoder_model = TransformerDecoder(config)\n","decoder_output = decoder_model(x_tgt_ids, enc_output, x_src_ids)\n","print(decoder_output.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"n9pa_DrNHt5Q","executionInfo":{"status":"ok","timestamp":1711967257442,"user_tz":-540,"elapsed":1231,"user":{"displayName":"김수성","userId":"09570314636293598107"}},"outputId":"82672ceb-0d6c-4618-8d63-824369171ea5"},"execution_count":209,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([1, 10, 768])\n","torch.Size([1, 10, 768])\n"]}]},{"cell_type":"code","source":["print(config)\n","print(inputs.input_ids)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LVcr56kKEXtJ","executionInfo":{"status":"ok","timestamp":1711967260783,"user_tz":-540,"elapsed":324,"user":{"displayName":"김수성","userId":"09570314636293598107"}},"outputId":"2c5618d3-b51f-4fa8-d6e9-c2cf0dcc0882"},"execution_count":210,"outputs":[{"output_type":"stream","name":"stdout","text":["BertConfig {\n","  \"_name_or_path\": \"bert-base-uncased\",\n","  \"architectures\": [\n","    \"BertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_length\": 15,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.38.2\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","tensor([[  101,  2051, 10029,  2066,  2019,  8612,   102,     0,     0,     0]])\n"]}]},{"cell_type":"code","source":["import copy\n","\n","class Transformer(nn.Module):\n","  def __init__(self, config, apply_softmax):\n","    super().__init__()\n","    self.encoder = TransformerEncoder(config)\n","    self.decoder = TransformerDecoder(config)\n","\n","    hidden_size = config.hidden_size\n","    vocab_size = config.vocab_size\n","    self.embed_to_tokens = nn.Linear(hidden_size, vocab_size)\n","    self.apply_softmax=apply_softmax\n","\n","  def forward(self, x_enc, x_tgt):\n","    enc_ids = copy.deepcopy(x_enc)\n","    enc_outputs = self.encoder(x_enc)\n","\n","    #enc_id가 담긴 입력을 Decoder에 넘겨줘야함.\n","    outputs = self.decoder(x_tgt, enc_outputs, enc_ids)\n","    outputs = self.embed_to_tokens(outputs)\n","\n","    if self.apply_softmax:\n","      outputs = F.softmax(outputs, dim=-1)\n","    return outputs"],"metadata":{"id":"mIVPVRUd5MFG","executionInfo":{"status":"ok","timestamp":1711967263387,"user_tz":-540,"elapsed":7,"user":{"displayName":"김수성","userId":"09570314636293598107"}}},"execution_count":211,"outputs":[]},{"cell_type":"code","source":["def get_src_tgt_input(source_text, target_text, config):\n","  source_inputs = tokenizer(source_text, padding=True, max_length=config.max_length, return_tensors=\"pt\", truncation=True)\n","  target_inputs = tokenizer(source_text, padding=True, max_length=config.max_length, return_tensors=\"pt\", truncation=True)\n","  source_inputs.input_ids = get_padded_sequence(source_inputs.input_ids, config.max_length)\n","  target_inputs.input_ids = get_padded_sequence(target_inputs.input_ids, config.max_length)\n","  return source_inputs, target_inputs"],"metadata":{"id":"1Em9jMhCttnT","executionInfo":{"status":"ok","timestamp":1711967009649,"user_tz":-540,"elapsed":4,"user":{"displayName":"김수성","userId":"09570314636293598107"}}},"execution_count":193,"outputs":[]},{"cell_type":"code","source":["source_text = \"I like apples.\"\n","target_text = \"I like apples, too.\"\n","config.max_length = 15\n","src_inputs, tgt_inputs = get_src_tgt_input(source_text, target_text, config)\n","\n","print(src_inputs.keys())\n","print(tgt_inputs.keys())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SbHAWHULK46A","executionInfo":{"status":"ok","timestamp":1711967265732,"user_tz":-540,"elapsed":6,"user":{"displayName":"김수성","userId":"09570314636293598107"}},"outputId":"059f8187-5d28-4ea4-b9c0-6e98ece2678d"},"execution_count":212,"outputs":[{"output_type":"stream","name":"stdout","text":["dict_keys(['input_ids', 'token_type_ids', 'attention_mask'])\n","dict_keys(['input_ids', 'token_type_ids', 'attention_mask'])\n"]}]},{"cell_type":"code","source":["# @title model 생성\n","torch.manual_seed(0) # seed를 고정하여 재현성 조정.\n","final_model = Transformer(config, apply_softmax=True)"],"metadata":{"id":"bRu6InT_L8vx","executionInfo":{"status":"ok","timestamp":1711967646107,"user_tz":-540,"elapsed":2824,"user":{"displayName":"김수성","userId":"09570314636293598107"}}},"execution_count":224,"outputs":[]},{"cell_type":"code","source":["outputs =final_model(src_inputs.input_ids, tgt_inputs.input_ids)\n","print(outputs.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dNOqipPvMMUS","executionInfo":{"status":"ok","timestamp":1711967646494,"user_tz":-540,"elapsed":392,"user":{"displayName":"김수성","userId":"09570314636293598107"}},"outputId":"2f734e77-bb26-4e54-eb9d-d46f6f45c810"},"execution_count":225,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([1, 15, 768])\n","torch.Size([1, 15, 30522])\n"]}]},{"cell_type":"code","source":["# @title Predicted Sequence 출력(학습이 안되어 이상한게 나옴.)\n","\n","predicted_indices = torch.argmax(outputs, dim=-1)\n","print(predicted_indices)\n","predicted_tokens = tokenizer.convert_ids_to_tokens(predicted_indices.squeeze().tolist())\n","\n","print(\"추론된 토큰 시퀀스:\")\n","print(predicted_tokens)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3OlWBtfhMuDb","executionInfo":{"status":"ok","timestamp":1711967683320,"user_tz":-540,"elapsed":6,"user":{"displayName":"김수성","userId":"09570314636293598107"}},"outputId":"a1b90bf0-c88c-431f-834e-744bb7a98c0d"},"execution_count":228,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[29064, 19550, 22943, 14759, 16859, 28962, 18437, 15701,  3078, 20809,\n","         19667, 17010, 27580, 15508,  9812]])\n","추론된 토큰 시퀀스:\n","['##tsk', 'runoff', '##roid', 'drowning', 'knob', 'pebbles', 'muse', 'shouts', 'primary', 'congregational', 'darlington', 'tilt', '##cliff', 'worries', 'dimension']\n"]}]}]}